{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import  DataLoader\n",
    "from Exp_DataSet import Corpus\n",
    "from Exp_Model import BiLSTM_model, Transformer_model\n",
    "import torch.nn.functional as F\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "class Transformer_model(nn.Module):\n",
    "    def __init__(self, vocab_size, ntoken, d_emb=512, d_hid=2048, nhead=8, nlayers=6, dropout=0.2, embedding_weight=None):\n",
    "        super(Transformer_model, self).__init__()\n",
    "        # 将\"预训练的词向量\"整理成 token->embedding 的二维映射矩阵 emdedding_weight 的形式，初始化 _weight\n",
    "        # 当 emdedding_weight == None 时，表示随机初始化\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_emb, _weight=embedding_weight)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_emb, max_len=ntoken)\n",
    "        self.encode_layer = nn.TransformerEncoderLayer(d_model=d_emb, nhead=nhead, dim_feedforward=d_hid)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=self.encode_layer, num_layers=nlayers)\n",
    "        #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "        # 请自行设计对 transformer 隐藏层数据的处理和选择方法\n",
    "        self.dropout = nn.Dropout(dropout)  # 可选\n",
    "        self.ntoken = ntoken\n",
    "        self.d_hid = d_hid\n",
    "        self.d_emb = d_emb\n",
    "        # 请自行设计分类器\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.d_emb, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 15),\n",
    "        )\n",
    "\n",
    "        #------------------------------------------------------end------------------------------------------------------#\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.embed(x)     \n",
    "        x = x.permute(1, 0, 2)          \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)      # [batch_size, ntoken, d_emb]\n",
    "        #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "        # 对 transformer_encoder 的隐藏层输出进行处理和选择，并完成分类\n",
    "        x = self.dropout(x) # 可选\n",
    "        #x = x.reshape(-1, self.ntoken*self.d_emb) \n",
    "        #x = F.avg_pool1d(x.permute(0,2,1), x.size(1)).squeeze()   # 池化并挤压后[batch_size, d_emb]\n",
    "        #取最后两个时间步的输出，然后作池化，然后分类\n",
    "        x = torch.cat((x[:, -1, :], x[:, -2, :]), dim=1)\n",
    "        x = F.max_pool1d(x.permute(0,2,1), x.size(1)).squeeze()\n",
    "        x = self.fc(x)\n",
    "        #------------------------------------------------------end------------------------------------------------------#\n",
    "        return x\n",
    "    \n",
    "class BiLSTM_model(nn.Module):\n",
    "    \"\"\"\n",
    "    vocab_size: 词表大小,不是词向量表中词的个数，而是数据集中切出来的所有词的个数\n",
    "    ntoken: 一个句子中的词的最大个数\n",
    "    d_emb: 词向量的维度\n",
    "    d_hid: 隐藏层的维度\n",
    "    nlayers: lstm层数\n",
    "    dropout: dropout的比例\n",
    "    embedding_weight: 预训练的词向量，格式为 token->embedding 的二维映射矩阵\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, ntoken, d_emb=100, d_hid=80, nlayers=1, dropout=0.2, embedding_weight=None):\n",
    "        super(BiLSTM_model, self).__init__()\n",
    "        # 将\"预训练的词向量\"整理成 token->embedding 的二维映射矩阵 emdedding_weight 的形式，初始化 _weight\n",
    "        # 当 emdedding_weight == None 时，表示随机初始化\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_emb, _weight=embedding_weight)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=d_emb, hidden_size=d_hid, num_layers=nlayers, bidirectional=True, batch_first=True)\n",
    "        #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "        # 请自行设计对 bilstm 隐藏层数据的处理和选择方法\n",
    "        self.dropout = nn.Dropout(dropout)  # 可选\n",
    "        self.ntoken = ntoken\n",
    "        self.d_hid = d_hid\n",
    "        # 请自行设计分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_hid*2, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 15),\n",
    "        )\n",
    "       \n",
    "        #------------------------------------------------------end------------------------------------------------------#\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        # x = x.long()\n",
    "        #print(\"输入embed的x:\",type(x), x.shape)\n",
    "        x = self.embed(x)\n",
    "        x = self.lstm(x)[0]\n",
    "        #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "        # 对 bilstm 的隐藏层输出进行处理和选择，并完成分类\n",
    "        #x = self.dropout(x).reshape(-1, self.ntoken*self.d_hid*2)   # ntoken*nhid*2 (2 means bidirectional)\n",
    "        #先做池化，然后在分类\n",
    "        #x = F.max_pool1d(x.permute(0,2,1), x.size(1)).squeeze()\n",
    "        #取最后两个时间步的输出，然后作池化，然后分类\n",
    "        x = torch.cat((x[:, -1, :], x[:, -2, :]), dim=1)\n",
    "        x = F.max_pool1d(x.permute(0,2,1), x.size(1)).squeeze()\n",
    "        x = self.classifier(x)\n",
    "        #------------------------------------------------------end------------------------------------------------------#\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_folder = './data/tnews_public'\n",
    "    output_folder = './output'\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "    # 以下为超参数，可根据需要修改\n",
    "        # 每个词向量的维度\n",
    "    max_token_per_sent = 50 # 每个句子预设的最大 token 数\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    lr = 1e-4\n",
    "    #------------------------------------------------------end------------------------------------------------------#\n",
    "\n",
    "    dataset = Corpus(dataset_folder, max_token_per_sent)\n",
    "\n",
    "    embedding_dim = dataset.embedding_weight.shape[1] # 词向量维度 \n",
    "\n",
    "    vocab_size = len(dataset.dictionary.tkn2word) # 词表大小\n",
    "\n",
    "    data_loader_train = DataLoader(dataset=dataset.train, batch_size=batch_size, shuffle=True)\n",
    "    data_loader_valid = DataLoader(dataset=dataset.valid, batch_size=batch_size, shuffle=False)\n",
    "    data_loader_test = DataLoader(dataset=dataset.test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "#     # 可修改选择的模型以及传入的参数\n",
    "# model = BiLSTM_model(vocab_size=vocab_size,\n",
    "#                      ntoken=max_token_per_sent,\n",
    "#                      d_emb=embedding_dim,\n",
    "#                      embedding_weight=dataset.embedding_weight # 使用预训练的词向量，需传入 embedding_weight\n",
    "#                      ).to(device)     \n",
    "    \n",
    "model = Transformer_model(vocab_size=vocab_size,\n",
    "                         ntoken=max_token_per_sent,\n",
    "                         d_emb=embedding_dim,\n",
    "                         nhead=5,#head需要整除d_emb，300除以5等于60\n",
    "                         #d_hid=80,\n",
    "                         embedding_weight=dataset.embedding_weight # 使用预训练的词向量，需传入 embedding_weight\n",
    "                         ).to(device)         \n",
    "    #------------------------------------------------------end------------------------------------------------------#\n",
    "    \n",
    "    # 设置损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "    # 设置优化器                                       \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1668/1668 [00:43<00:00, 38.02it/s, acc=0.788, loss=0.707]\n",
      "100%|██████████| 313/313 [00:01<00:00, 240.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.7071, train accuracy: 78.82%, valid accuracy: 34.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1668/1668 [00:42<00:00, 38.91it/s, acc=0.806, loss=0.664]\n",
      "100%|██████████| 313/313 [00:01<00:00, 245.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 0.6640, train accuracy: 80.58%, valid accuracy: 36.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1668/1668 [00:43<00:00, 38.59it/s, acc=0.819, loss=0.629]\n",
      "100%|██████████| 313/313 [00:01<00:00, 243.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train loss: 0.6287, train accuracy: 81.87%, valid accuracy: 36.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1668/1668 [00:44<00:00, 37.40it/s, acc=0.818, loss=0.624]\n",
      "100%|██████████| 313/313 [00:01<00:00, 224.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, train loss: 0.6242, train accuracy: 81.82%, valid accuracy: 35.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1668/1668 [00:43<00:00, 38.14it/s, acc=0.816, loss=0.634]\n",
      "100%|██████████| 313/313 [00:01<00:00, 242.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, train loss: 0.6340, train accuracy: 81.56%, valid accuracy: 36.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 进行训练\n",
    "def valid():\n",
    "    '''\n",
    "    进行验证，返回模型在验证集上的 accuracy\n",
    "    '''\n",
    "    total_true = []\n",
    "\n",
    "    model.eval() #这句话在测试之前使用，不启用 BatchNormalization 和 Dropout\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader_valid, dynamic_ncols=True):\n",
    "            batch_x, batch_y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            y_hat = model(batch_x)\n",
    "            # 取分类概率最大的类别作为预测的类别\n",
    "            y_hat = torch.tensor([torch.argmax(_) for _ in y_hat]).to(device)\n",
    "\n",
    "            total_true.append(torch.sum(y_hat == batch_y).item())\n",
    "\n",
    "        return sum(total_true) / (batch_size * len(total_true))\n",
    "\n",
    "def train():\n",
    "\n",
    "    max_valid_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  #这一句话是为了启用 BatchNormalization 和 Dropout，一定要在训练前调用，否则会有影响\n",
    "\n",
    "        total_loss = []\n",
    "        total_true = []\n",
    "\n",
    "        tqdm_iterator = tqdm(data_loader_train, dynamic_ncols=True, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        for data in tqdm_iterator:\n",
    "            # 选取对应批次数据的输入和标签\n",
    "            batch_x, batch_y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # 模型预测\n",
    "            #------------------------------------change------------------------------------#\n",
    "            y_hat = model(batch_x)\n",
    "            #------------------------------------endOfChange------------------------------------#\n",
    "            loss = loss_function(y_hat, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()   # 梯度清零\n",
    "            loss.backward()         # 计算梯度\n",
    "            optimizer.step()        # 更新参数\n",
    "\n",
    "            y_hat = torch.tensor([torch.argmax(_) for _ in y_hat]).to(device)\n",
    "            \n",
    "            total_true.append(torch.sum(y_hat == batch_y).item())\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            tqdm_iterator.set_postfix(loss=sum(total_loss) / len(total_loss),\n",
    "                                      acc=sum(total_true) / (batch_size * len(total_true)))\n",
    "        \n",
    "        tqdm_iterator.close()\n",
    "\n",
    "        train_loss = sum(total_loss) / len(total_loss)\n",
    "        train_acc = sum(total_true) / (batch_size * len(total_true))\n",
    "\n",
    "        valid_acc = valid()\n",
    "\n",
    "        #if valid_acc > max_valid_acc:\n",
    "           # torch.save(model, os.path.join(output_folder, \"model.ckpt\"))\n",
    "\n",
    "        print(f\"epoch: {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_acc*100:.2f}%, valid accuracy: {valid_acc*100:.2f}%\")\n",
    "\n",
    "train()\n",
    "\n",
    "    # 对测试集进行预测\n",
    "def predict():\n",
    "    '''\n",
    "    读取训练好的模型对测试集进行预测，并生成结果文件\n",
    "    '''\n",
    "    test_ids = [] \n",
    "    test_pred = []\n",
    "\n",
    "    model = torch.load(os.path.join(output_folder, \"model.ckpt\")).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader_test, dynamic_ncols=True): \n",
    "            batch_x, batch_y = data[0].to(device), data[1]\n",
    "\n",
    "            y_hat = model(batch_x)\n",
    "            y_hat = torch.tensor([torch.argmax(_) for _ in y_hat])\n",
    "\n",
    "            test_ids += batch_y.tolist()\n",
    "            test_pred += y_hat.tolist()\n",
    "\n",
    "    # 写入文件\n",
    "    with open(os.path.join(output_folder, \"predict.json\"), \"w\") as f:\n",
    "        for idx, label_idx in enumerate(test_pred):\n",
    "            one_data = {}\n",
    "            one_data[\"id\"] = test_ids[idx]\n",
    "            one_data[\"pred_label_desc\"] = dataset.dictionary.idx2label[label_idx][1]\n",
    "            json_data = json.dumps(one_data)    # 将字典转为json格式的字符串\n",
    "            f.write(json_data + \"\\n\")\n",
    "\n",
    "\n",
    "            \n",
    "#predict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
