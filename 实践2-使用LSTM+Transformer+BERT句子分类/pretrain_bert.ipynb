{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import  DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.word2tkn = {\"[PAD]\": 0}\n",
    "        self.tkn2word = [\"[PAD]\"]\n",
    "\n",
    "        self.label2idx = {} #内容为 label->idx 的映射\n",
    "        self.idx2label = [] #内容为 [label, label_desc] 的列表\n",
    "\n",
    "        # 获取 label 的 映射\n",
    "        with open(os.path.join(path, 'labels.json'), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                one_data = json.loads(line)\n",
    "                label, label_desc = one_data['label'], one_data['label_desc']\n",
    "                self.idx2label.append([label, label_desc])\n",
    "                self.label2idx[label] = len(self.idx2label) - 1\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2tkn:\n",
    "            self.tkn2word.append(word)\n",
    "            self.word2tkn[word] = len(self.tkn2word) - 1\n",
    "        return self.word2tkn[word]\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    '''\n",
    "    完成对数据集的读取和预处理，处理后得到所有文本数据的对应的 token 表示及相应的标签。\n",
    "    \n",
    "    该类适用于任务一、任务二，若要完成任务三，需对整个类进行调整，例如，可直接调用预训练模型提供的 tokenizer 将文本转为对应的 token 序列。\n",
    "    '''\n",
    "    def __init__(self, path, max_token_per_sent):\n",
    "        self.dictionary = Dictionary(path)\n",
    "\n",
    "        self.max_token_per_sent = max_token_per_sent\n",
    "\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.json'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'dev.json'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.json'), True)\n",
    "       \n",
    "\n",
    "        #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "        # 若要采用预训练的 embedding, 需处理得到 token->embedding 的映射矩阵 embedding_weight。矩阵的格式参考 nn.Embedding() 中的参数 _weight\n",
    "        # 注意，需考虑 [PAD] 和 [UNK] 两个特殊词向量的设置\n",
    "        # word2vector = KeyedVectors.load_word2vec_format(os.path.join(path,'sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'))\n",
    "        # embedding_dim = word2vector.vector_size\n",
    "        # embedding_weight = np.zeros((len(self.dictionary.tkn2word), embedding_dim))\n",
    "        # for word, token in self.dictionary.word2tkn.items():\n",
    "        #     if word in word2vector:\n",
    "        #         embedding_weight[token] = word2vector[word]\n",
    "        #     else:\n",
    "        #         embedding_weight[token] = np.random.uniform(-0.01, 0.01, embedding_dim).astype(\"float32\")\n",
    "        # self.embedding_weight = torch.tensor(embedding_weight, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        #把train, valid, test 中的label换乘词向量\n",
    "        #不需要这一步，在LSTM_Model中会自动转换\n",
    "        # for i in range(len(self.train)):\n",
    "        #     self.train[i][1] = self.embedding_weight[self.train[i][1]]\n",
    "        # for i in range(len(self.valid)):\n",
    "        #     self.valid[i][1] = self.embedding_weight[self.valid[i][1]]\n",
    "        # for i in range(len(self.test)):\n",
    "        #     self.test[i][1] = self.embedding_weight[self.test[i][1]]\n",
    "        #------------------------------------------------------end------------------------------------------------------#\n",
    "\n",
    "    def pad(self, origin_token_seq):\n",
    "        '''\n",
    "        padding: 将原始的 token 序列补 0 至预设的最大长度 self.max_token_per_sent\n",
    "        '''\n",
    "        if len(origin_token_seq) > self.max_token_per_sent:\n",
    "            return origin_token_seq[:self.max_token_per_sent]\n",
    "        else:\n",
    "            return origin_token_seq + [0 for _ in range(self.max_token_per_sent-len(origin_token_seq))]\n",
    "\n",
    "    def tokenize(self, path, test_mode=False):\n",
    "        '''\n",
    "        处理指定的数据集分割，处理后每条数据中的 sentence 都将转化成对应的 token 序列。\n",
    "        '''\n",
    "        idss = []\n",
    "        labels = []\n",
    "        maskss = []\n",
    "        tokenizer = BertTokenizer.from_pretrained('./bert-base-chinese')\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                one_data = json.loads(line)  # 读取一条数据\n",
    "                sent = one_data['sentence']\n",
    "                #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "                # 若要采用预训练的 embedding, 需在此处对 sent 进行分词\n",
    "\n",
    "                \n",
    "                tokens = tokenizer(sent, return_tensors=None, truncation=True, padding=\"max_length\",max_length=self.max_token_per_sent)\n",
    "                input_ids = tokens['input_ids']\n",
    "                attention_mask = tokens['attention_mask']\n",
    "                if test_mode:\n",
    "                    label = json.loads(line)['id']      \n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    label = json.loads(line)['label']\n",
    "                    labels.append(self.dictionary.label2idx[label])\n",
    "\n",
    "\n",
    "                \n",
    "                idss.append(input_ids)\n",
    "                maskss.append(attention_mask)\n",
    "                \n",
    "                \n",
    "            idss = torch.tensor(np.array(idss))\n",
    "            maskss = torch.tensor(np.array(maskss))\n",
    "            labels = torch.tensor(np.array(labels)).long()\n",
    "             #idss的内容格式：[ ids1,id2....    ]\n",
    "             #labels的内容格式：[label1,label2....]\n",
    "             #TensorDataset是一个pytorch的包装数据的类，可以把数据包装成TensorDataset的形式，然后再放入DataLoader中\n",
    "             #格式是 (ids1, label1)元素组成的数据集\n",
    "        return TensorDataset(idss,labels, maskss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_folder = './data/tnews_public'\n",
    "    output_folder = './output'\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "    # 以下为超参数，可根据需要修改\n",
    "        # 每个词向量的维度\n",
    "    max_token_per_sent = 50 # 每个句子预设的最大 token 数\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    lr = 1e-4\n",
    "    #------------------------------------------------------end------------------------------------------------------#\n",
    "\n",
    "    dataset = Corpus(dataset_folder, max_token_per_sent)\n",
    "\n",
    "    #embedding_dim = dataset.embedding_weight.shape[1] # 词向量维度 \n",
    "\n",
    "    #vocab_size = len(dataset.dictionary.tkn2word) # 词表大小\n",
    "\n",
    "    data_loader_train = DataLoader(dataset=dataset.train, batch_size=batch_size, shuffle=True)\n",
    "    data_loader_valid = DataLoader(dataset=dataset.valid, batch_size=batch_size, shuffle=False)\n",
    "    data_loader_test = DataLoader(dataset=dataset.test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "      \n",
    "    #------------------------------------------------------end------------------------------------------------------#\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        print(\"Loading Model...\")\n",
    "        self.bert = BertModel.from_pretrained('./bert-base-chinese')\n",
    "        print(\"Loaded.\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 15),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(x, attention_mask=mask)[0][:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 初始化BERT分类器\n",
    "num_labels = 15  # 根据具体任务的类别数量设置\n",
    "model = Bert_Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   5%|▌         | 90/1668 [00:22<06:25,  4.09it/s, acc=0.528, loss=1.39]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\lib.fanfan\\study\\NLP_EXPERIMENT\\实践2_1120212297_周帆\\pretrain_bert.ipynb 单元格 5\u001b[0m line \u001b[0;36m<cell line: 78>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         \u001b[39m#if valid_acc > max_valid_acc:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m            \u001b[39m# torch.save(model, os.path.join(output_folder, \"model.ckpt\"))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, train loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, train accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, valid accuracy: \u001b[39m\u001b[39m{\u001b[39;00mvalid_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m train()\n",
      "\u001b[1;32me:\\lib.fanfan\\study\\NLP_EXPERIMENT\\实践2_1120212297_周帆\\pretrain_bert.ipynb 单元格 5\u001b[0m line \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()         \u001b[39m# 计算梯度\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()        \u001b[39m# 更新参数\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m y_hat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([torch\u001b[39m.\u001b[39;49margmax(_) \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m y_hat])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m total_true\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39msum(y_hat \u001b[39m==\u001b[39m batch_y)\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/NLP_EXPERIMENT/%E5%AE%9E%E8%B7%B52_1120212297_%E5%91%A8%E5%B8%86/pretrain_bert.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m total_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def valid():\n",
    "    '''\n",
    "    进行验证，返回模型在验证集上的 accuracy\n",
    "    '''\n",
    "    total_true = []\n",
    "\n",
    "    model.eval() #这句话在测试之前使用，不启用 BatchNormalization 和 Dropout\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader_valid, dynamic_ncols=True):\n",
    "           # print(data)\n",
    "            batch_x, batch_y ,batch_z = data[0].to(device), data[1].to(device),data[2].to(device)\n",
    "\n",
    "            y_hat = model(batch_x,batch_z)\n",
    "            # 取分类概率最大的类别作为预测的类别\n",
    "            y_hat = torch.tensor([torch.argmax(_) for _ in y_hat]).to(device)\n",
    "\n",
    "            total_true.append(torch.sum(y_hat == batch_y).item())\n",
    "\n",
    "        return sum(total_true) / (batch_size * len(total_true))\n",
    "\n",
    "# 设置损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# 设置优化器,冻结预训练的参数                    \n",
    "for name, param in model.named_parameters():\n",
    "    if \"bert\" in name:\n",
    "        param.requires_grad = False  \n",
    "    \n",
    "#设置优化器时，只传入最后一层分类层的参数           \n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=lr, weight_decay=5e-4)  \n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    max_valid_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  #这一句话是为了启用 BatchNormalization 和 Dropout，一定要在训练前调用，否则会有影响\n",
    "\n",
    "        total_loss = []\n",
    "        total_true = []\n",
    "\n",
    "        tqdm_iterator = tqdm(data_loader_train, dynamic_ncols=True, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        for data in tqdm_iterator:\n",
    "            # 选取对应批次数据的输入、maskss和标签\n",
    "            batch_x, batch_y ,batch_z= data[0].to(device), data[1].to(device),data[2].to(device)\n",
    "\n",
    "            # 模型预测\n",
    "            #------------------------------------change------------------------------------#\n",
    "            y_hat = model(batch_x,batch_z)\n",
    "            #------------------------------------endOfChange------------------------------------#\n",
    "            loss = loss_function(y_hat, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()   # 梯度清零\n",
    "            loss.backward()         # 计算梯度\n",
    "            optimizer.step()        # 更新参数\n",
    "\n",
    "            y_hat = torch.tensor([torch.argmax(_) for _ in y_hat]).to(device)\n",
    "            \n",
    "            total_true.append(torch.sum(y_hat == batch_y).item())\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            tqdm_iterator.set_postfix(loss=sum(total_loss) / len(total_loss),\n",
    "                                      acc=sum(total_true) / (batch_size * len(total_true)))\n",
    "        \n",
    "        tqdm_iterator.close()\n",
    "\n",
    "        train_loss = sum(total_loss) / len(total_loss)\n",
    "        train_acc = sum(total_true) / (batch_size * len(total_true))\n",
    "\n",
    "        valid_acc = valid()\n",
    "\n",
    "        #if valid_acc > max_valid_acc:\n",
    "           # torch.save(model, os.path.join(output_folder, \"model.ckpt\"))\n",
    "\n",
    "        print(f\"epoch: {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_acc*100:.2f}%, valid accuracy: {valid_acc*100:.2f}%\")\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
