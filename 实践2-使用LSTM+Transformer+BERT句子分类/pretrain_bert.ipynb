{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86185\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import  DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.word2tkn = {\"[PAD]\": 0}\n",
    "        self.tkn2word = [\"[PAD]\"]\n",
    "\n",
    "        self.label2idx = {} #内容为 label->idx 的映射\n",
    "        self.idx2label = [] #内容为 [label, label_desc] 的列表\n",
    "\n",
    "        # 获取 label 的 映射\n",
    "        with open(os.path.join(path, 'labels.json'), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                one_data = json.loads(line)\n",
    "                label, label_desc = one_data['label'], one_data['label_desc']\n",
    "                self.idx2label.append([label, label_desc])\n",
    "                self.label2idx[label] = len(self.idx2label) - 1\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2tkn:\n",
    "            self.tkn2word.append(word)\n",
    "            self.word2tkn[word] = len(self.tkn2word) - 1\n",
    "        return self.word2tkn[word]\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    '''\n",
    "    完成对数据集的读取和预处理，处理后得到所有文本数据的对应的 token 表示及相应的标签。\n",
    "    \n",
    "    该类适用于任务一、任务二，若要完成任务三，需对整个类进行调整，例如，可直接调用预训练模型提供的 tokenizer 将文本转为对应的 token 序列。\n",
    "    '''\n",
    "    def __init__(self, path, max_token_per_sent):\n",
    "        self.dictionary = Dictionary(path)\n",
    "\n",
    "        self.max_token_per_sent = max_token_per_sent\n",
    "\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.json'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'dev.json'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.json'), True)\n",
    "       \n",
    "\n",
    "        #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "        # 若要采用预训练的 embedding, 需处理得到 token->embedding 的映射矩阵 embedding_weight。矩阵的格式参考 nn.Embedding() 中的参数 _weight\n",
    "        # 注意，需考虑 [PAD] 和 [UNK] 两个特殊词向量的设置\n",
    "        word2vector = KeyedVectors.load_word2vec_format(os.path.join(path,'sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'))\n",
    "        embedding_dim = word2vector.vector_size\n",
    "        embedding_weight = np.zeros((len(self.dictionary.tkn2word), embedding_dim))\n",
    "        for word, token in self.dictionary.word2tkn.items():\n",
    "            if word in word2vector:\n",
    "                embedding_weight[token] = word2vector[word]\n",
    "            else:\n",
    "                embedding_weight[token] = np.random.uniform(-0.01, 0.01, embedding_dim).astype(\"float32\")\n",
    "        self.embedding_weight = torch.tensor(embedding_weight, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        #把train, valid, test 中的label换乘词向量\n",
    "        #不需要这一步，在LSTM_Model中会自动转换\n",
    "        # for i in range(len(self.train)):\n",
    "        #     self.train[i][1] = self.embedding_weight[self.train[i][1]]\n",
    "        # for i in range(len(self.valid)):\n",
    "        #     self.valid[i][1] = self.embedding_weight[self.valid[i][1]]\n",
    "        # for i in range(len(self.test)):\n",
    "        #     self.test[i][1] = self.embedding_weight[self.test[i][1]]\n",
    "        #------------------------------------------------------end------------------------------------------------------#\n",
    "\n",
    "    def pad(self, origin_token_seq):\n",
    "        '''\n",
    "        padding: 将原始的 token 序列补 0 至预设的最大长度 self.max_token_per_sent\n",
    "        '''\n",
    "        if len(origin_token_seq) > self.max_token_per_sent:\n",
    "            return origin_token_seq[:self.max_token_per_sent]\n",
    "        else:\n",
    "            return origin_token_seq + [0 for _ in range(self.max_token_per_sent-len(origin_token_seq))]\n",
    "\n",
    "    def tokenize(self, path, test_mode=False):\n",
    "        '''\n",
    "        处理指定的数据集分割，处理后每条数据中的 sentence 都将转化成对应的 token 序列。\n",
    "        '''\n",
    "        idss = []\n",
    "        labels = []\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                one_data = json.loads(line)  # 读取一条数据\n",
    "                sent = one_data['sentence']\n",
    "                #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "                # 若要采用预训练的 embedding, 需在此处对 sent 进行分词\n",
    "\n",
    "                tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "                tokens = tokenizer(sent, return_tensors='pt', truncation=True, padding=True)\n",
    "                input_ids = tokens['input_ids']\n",
    "                attention_mask = tokens['attention_mask']\n",
    "\n",
    "                \n",
    "                idss = []\n",
    "                idss.append(input_ids)\n",
    "                maskss = []\n",
    "                maskss.append(attention_mask)\n",
    "                labels = []\n",
    "                labels.append(self.dictionary.label2idx[one_data['label']])  \n",
    "                \n",
    "            idss = torch.tensor(np.array(idss))\n",
    "            maskss = torch.tensor(np.array(maskss))\n",
    "            labels = torch.tensor(np.array(labels)).long()\n",
    "             #idss的内容格式：[ ids1,id2....    ]\n",
    "             #labels的内容格式：[label1,label2....]\n",
    "             #TensorDataset是一个pytorch的包装数据的类，可以把数据包装成TensorDataset的形式，然后再放入DataLoader中\n",
    "             #格式是 (ids1, label1)元素组成的数据集\n",
    "        return TensorDataset(idss, labels,maskss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_folder = './data/tnews_public'\n",
    "    output_folder = './output'\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    #-----------------------------------------------------begin-----------------------------------------------------#\n",
    "    # 以下为超参数，可根据需要修改\n",
    "        # 每个词向量的维度\n",
    "    max_token_per_sent = 50 # 每个句子预设的最大 token 数\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    lr = 1e-4\n",
    "    #------------------------------------------------------end------------------------------------------------------#\n",
    "\n",
    "    dataset = Corpus(dataset_folder, max_token_per_sent)\n",
    "\n",
    "    embedding_dim = dataset.embedding_weight.shape[1] # 词向量维度 \n",
    "\n",
    "    vocab_size = len(dataset.dictionary.tkn2word) # 词表大小\n",
    "\n",
    "    data_loader_train = DataLoader(dataset=dataset.train, batch_size=batch_size, shuffle=True)\n",
    "    data_loader_valid = DataLoader(dataset=dataset.valid, batch_size=batch_size, shuffle=False)\n",
    "    data_loader_test = DataLoader(dataset=dataset.test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "      \n",
    "    #------------------------------------------------------end------------------------------------------------------#\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 初始化BERT分类器\n",
    "num_labels = 15  # 根据具体任务的类别数量设置\n",
    "model = BertClassifier(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def valid():\n",
    "    '''\n",
    "    进行验证，返回模型在验证集上的 accuracy\n",
    "    '''\n",
    "    total_true = []\n",
    "\n",
    "    model.eval() #这句话在测试之前使用，不启用 BatchNormalization 和 Dropout\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader_valid, dynamic_ncols=True):\n",
    "            print(data)\n",
    "            batch_x, batch_y ,batch_z = data[0].to(device), data[1].to(device),data[2].to(device)\n",
    "\n",
    "            y_hat = model(batch_x,batch_z)\n",
    "            # 取分类概率最大的类别作为预测的类别\n",
    "            y_hat = torch.tensor([torch.argmax(_) for _ in y_hat]).to(device)\n",
    "\n",
    "            total_true.append(torch.sum(y_hat == batch_y).item())\n",
    "\n",
    "        return sum(total_true) / (batch_size * len(total_true))\n",
    "\n",
    "# 设置损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# 设置优化器,冻结预训练的参数                    \n",
    "for name, param in model.named_parameters():\n",
    "    if \"bert\" in name:\n",
    "        param.requires_grad = False  \n",
    "    if \"dropout\" in name:\n",
    "        param.requires_grad = False\n",
    "#设置优化器时，只传入最后一层分类层的参数           \n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=lr, weight_decay=5e-4)  \n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    max_valid_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  #这一句话是为了启用 BatchNormalization 和 Dropout，一定要在训练前调用，否则会有影响\n",
    "\n",
    "        total_loss = []\n",
    "        total_true = []\n",
    "\n",
    "        tqdm_iterator = tqdm(data_loader_train, dynamic_ncols=True, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        for data in tqdm_iterator:\n",
    "            # 选取对应批次数据的输入、maskss和标签\n",
    "            batch_x, batch_y ,batch_z= data[0].to(device), data[1].to(device),data[2].to(device)\n",
    "\n",
    "            # 模型预测\n",
    "            #------------------------------------change------------------------------------#\n",
    "            y_hat = model(batch_x,batch_y)\n",
    "            #------------------------------------endOfChange------------------------------------#\n",
    "            loss = loss_function(y_hat, batch_z)\n",
    "\n",
    "            optimizer.zero_grad()   # 梯度清零\n",
    "            loss.backward()         # 计算梯度\n",
    "            optimizer.step()        # 更新参数\n",
    "\n",
    "            y_hat = torch.tensor([torch.argmax(_) for _ in y_hat]).to(device)\n",
    "            \n",
    "            total_true.append(torch.sum(y_hat == batch_z).item())\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            tqdm_iterator.set_postfix(loss=sum(total_loss) / len(total_loss),\n",
    "                                      acc=sum(total_true) / (batch_size * len(total_true)))\n",
    "        \n",
    "        tqdm_iterator.close()\n",
    "\n",
    "        train_loss = sum(total_loss) / len(total_loss)\n",
    "        train_acc = sum(total_true) / (batch_size * len(total_true))\n",
    "\n",
    "        valid_acc = valid()\n",
    "\n",
    "        #if valid_acc > max_valid_acc:\n",
    "           # torch.save(model, os.path.join(output_folder, \"model.ckpt\"))\n",
    "\n",
    "        print(f\"epoch: {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_acc*100:.2f}%, valid accuracy: {valid_acc*100:.2f}%\")\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
